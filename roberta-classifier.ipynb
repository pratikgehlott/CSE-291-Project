{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-29T21:37:17.544343Z","iopub.status.busy":"2023-05-29T21:37:17.543404Z","iopub.status.idle":"2023-05-29T21:37:17.555025Z","shell.execute_reply":"2023-05-29T21:37:17.554086Z","shell.execute_reply.started":"2023-05-29T21:37:17.544306Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/project/wiki_csai.jsonl\n","/kaggle/input/project/open_qa.jsonl\n","/kaggle/input/project/dataset.csv\n","/kaggle/input/stackoverflow/merged_stackoverflow_18_34_Answers.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T21:37:17.561804Z","iopub.status.busy":"2023-05-29T21:37:17.559942Z","iopub.status.idle":"2023-05-29T21:37:17.574109Z","shell.execute_reply":"2023-05-29T21:37:17.573212Z","shell.execute_reply.started":"2023-05-29T21:37:17.561766Z"},"trusted":true},"outputs":[],"source":["# IMPORTS\n","import argparse\n","import json\n","import logging\n","import math\n","import torch\n","import os\n","import time\n","import random\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","import pickle\n","\n","\n","import datasets\n","from collections import Counter\n","from datasets import load_dataset, load_metric\n","from torch.utils.data import DataLoader\n","from torchmetrics.classification import MulticlassAccuracy, accuracy\n","from sklearn.metrics import classification_report, f1_score\n","from tqdm.auto import tqdm\n","from torch.utils.data import WeightedRandomSampler\n","import numpy as np\n","from datasets import Dataset, DatasetDict\n","\n","import transformers\n","from accelerate import Accelerator\n","from huggingface_hub import Repository\n","from transformers import (\n","    AdamW,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    PretrainedConfig,\n","    SchedulerType,\n","    default_data_collator,\n","    get_scheduler,\n","    set_seed,\n",")\n","from collections import Counter\n","from transformers.utils.versions import require_version"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T22:49:29.222950Z","iopub.status.busy":"2023-05-27T22:49:29.222280Z","iopub.status.idle":"2023-05-27T22:49:30.202743Z","shell.execute_reply":"2023-05-27T22:49:30.201559Z","shell.execute_reply.started":"2023-05-27T22:49:29.222913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T19:39:26.076062Z","iopub.status.busy":"2023-05-29T19:39:26.075654Z","iopub.status.idle":"2023-05-29T19:39:26.558386Z","shell.execute_reply":"2023-05-29T19:39:26.557385Z","shell.execute_reply.started":"2023-05-29T19:39:26.076018Z"},"trusted":true},"outputs":[],"source":["data_file_path = \"/kaggle/input/project/dataset.csv\"\n","output_dir=\"./\"\n","seed=42\n","\n","logger = logging.getLogger(__name__)\n","accelerator = Accelerator()\n","# Make one log on every process with the configuration for debugging.\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    level=logging.INFO,\n",")\n","logger.info(accelerator.state)\n","\n","# Setup logging, we only want one process per machine to log things on the screen.\n","# accelerator.is_local_main_process is only True for one process per machine.\n","logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n","if accelerator.is_local_main_process:\n","    datasets.utils.logging.set_verbosity_warning()\n","    transformers.utils.logging.set_verbosity_info()\n","else:\n","    datasets.utils.logging.set_verbosity_error()\n","    transformers.utils.logging.set_verbosity_error()\n","\n","# If passed along, set the training seed now.\n","if seed is not None:\n","    set_seed(seed)\n","\n","# Handle the repository creation\n","if accelerator.is_main_process:\n","    if output_dir is not None:\n","        os.makedirs(output_dir, exist_ok=True)\n","accelerator.wait_for_everyone()\n","\n","df= pd.read_csv(data_file_path)\n","    \n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-29T19:43:44.027359Z","iopub.status.busy":"2023-05-29T19:43:44.026876Z","iopub.status.idle":"2023-05-29T19:43:44.034780Z","shell.execute_reply":"2023-05-29T19:43:44.033681Z","shell.execute_reply.started":"2023-05-29T19:43:44.027323Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'To cook clams and mussels for pasta, add the shellfish to the sauce during the last 5-10 minutes of cooking (once the sauce has already been simmering for at least 30 minutes). The clams and mussels will release their Flavor into the sauce as they cook. Be sure to discard any shellfish that do not open after cooking.'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.chatgpt_answer[0]"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:34:41.954385Z","iopub.status.busy":"2023-05-28T00:34:41.953979Z","iopub.status.idle":"2023-05-28T00:34:41.962476Z","shell.execute_reply":"2023-05-28T00:34:41.961497Z","shell.execute_reply.started":"2023-05-28T00:34:41.954352Z"},"trusted":true},"outputs":[],"source":["df= df[df[\"origin\"]==\"AskCulinary\"]\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:34:42.822524Z","iopub.status.busy":"2023-05-28T00:34:42.821870Z","iopub.status.idle":"2023-05-28T00:34:42.989908Z","shell.execute_reply":"2023-05-28T00:34:42.989013Z","shell.execute_reply.started":"2023-05-28T00:34:42.822488Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","human_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['human_answer']}\" , axis=1)\n","chatgpt_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['chatgpt_answer']}\" , axis=1)\n","# human_answers= df[\"human_answer\"]\n","# chatgpt_answers = df[\"chatgpt_answer\"]\n","\n","processed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\n","\n","train_val_df, test_df= train_test_split(processed_df, test_size = 0.1, stratify=processed_df.label, random_state=0)\n","train_df, val_df = train_test_split(train_val_df, test_size = 0.1, stratify=train_val_df.label, random_state=0)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:34:45.945652Z","iopub.status.busy":"2023-05-28T00:34:45.945294Z","iopub.status.idle":"2023-05-28T00:34:50.887219Z","shell.execute_reply":"2023-05-28T00:34:50.886283Z","shell.execute_reply.started":"2023-05-28T00:34:45.945622Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.29.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n","loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.29.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}],"source":["sequence_classification_model=\"roberta-base\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(sequence_classification_model, do_lower_case=True)\n","if sequence_classification_model== \"gpt2\":\n","    tokenizer.padding_side = \"left\"\n","    tokenizer.pad_token = tokenizer.eos_token\n","    \n","    \n","train_encodings = tokenizer(train_df.text.tolist(), truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_df.text.tolist(), truncation=True, padding=True, max_length=512)\n","test_encodings = tokenizer(test_df.text.tolist(), truncation=True, padding=True, max_length=512)\n","\n","train_y = list(train_df['label'])\n","val_y = list(val_df['label'])\n","test_y = list(test_df['label'])\n","\n","\n","# Create PyTorch datasets\n","class TextClassificationDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}, torch.tensor(self.labels[idx])\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = TextClassificationDataset(train_encodings, train_y)\n","test_dataset = TextClassificationDataset(test_encodings, test_y)\n","val_dataset = TextClassificationDataset(val_encodings, val_y)\n","\n","# Create PyTorch data loaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:34:50.889831Z","iopub.status.busy":"2023-05-28T00:34:50.889447Z","iopub.status.idle":"2023-05-28T00:46:35.796220Z","shell.execute_reply":"2023-05-28T00:46:35.795071Z","shell.execute_reply.started":"2023-05-28T00:34:50.889787Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.29.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["The device is: cuda\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 476/476 [11:17<00:00,  1.42s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch: 0, Training Loss: 0.16957227854564896,     Training accuracy: 0.9180484693877551\n","Epoch 1:\n","  Training loss: 0.0962\n","  Testing loss: 0.0683\n","  Testing accuracy: 0.9775\n"]}],"source":["import torch\n","from tqdm import tqdm\n","import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.model_selection import train_test_split\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","# Load the pre-trained BERT model\n","model = AutoModelForSequenceClassification.from_pretrained(sequence_classification_model, num_labels=2)\n","if sequence_classification_model==\"gpt2\":\n","      model.config.pad_token_id = model.config.eos_token_id\n","\n","# Set the hyperparameters\n","learning_rate = 2e-5\n","weight_decay = 0.01\n","batch_size = 32\n","num_epochs = 1\n","\n","# Create the optimizer and the learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps = 1e-7)\n","total_steps = len(train_loader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=50, num_training_steps=total_steps)\n","\n","# Train the model\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(f\"The device is: {device}\")\n","model.to(device)\n","for epoch in range(num_epochs):\n","    total_train_loss = 0\n","    total_train_accuracy = 0\n","    model.train()\n","    pred_logits =torch.Tensor()\n","    for batch in tqdm(train_loader):\n","        optimizer.zero_grad()\n","        inputs = {key: val.to(device) for key, val in batch[0].items()}\n","        labels = batch[1].to(device)\n","        outputs = model(**inputs, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        logits = outputs.logits\n","        total_train_loss+=loss.item()\n","        softmax =torch.softmax(logits, dim=1)\n","        pred_logits= torch.cat([pred_logits, softmax.cpu()], dim=0)\n","\n","        preds = torch.argmax(logits, axis=1)\n","        train_accuracy = torch.sum(preds == labels).item() / len(labels)\n","        total_train_accuracy += train_accuracy\n","    torch.save(pred_logits,f\"train_roberta_{epoch}_ep_pred_tensors.pt\")\n","\n","    print(f\"epoch: {epoch}, Training Loss: {total_train_loss/len(train_loader)},\\\n","     Training accuracy: {total_train_accuracy / len(train_loader)}\")\n","    \n","    \n","    model.eval()\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    pred_class = []\n","    for batch in val_loader:\n","        with torch.no_grad():\n","            inputs = {key: val.to(device) for key, val in batch[0].items()}\n","            labels = batch[1].to(device)\n","            outputs = model(**inputs, labels=labels)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","        total_eval_loss += loss.item()\n","        preds = torch.argmax(logits, axis=1)\n","        eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n","    #     print(eval_accuracy)\n","        total_eval_accuracy += eval_accuracy\n","\n","    avg_eval_accuracy = total_eval_accuracy / len(val_loader)\n","    avg_eval_loss = total_eval_loss / len(val_loader)\n","\n","    print(f\"Epoch {epoch+1}:\")\n","    print(f\"  Training loss: {loss.item():.4f}\")\n","    print(f\"  Testing loss: {avg_eval_loss:.4f}\")\n","    print(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")\n","\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:46:35.798277Z","iopub.status.busy":"2023-05-28T00:46:35.797705Z","iopub.status.idle":"2023-05-28T00:47:04.199012Z","shell.execute_reply":"2023-05-28T00:47:04.197884Z","shell.execute_reply.started":"2023-05-28T00:46:35.798240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1:\n","  Training loss: 0.3812\n","  Testing loss: 0.0829\n","  Testing accuracy: 0.9701\n"]}],"source":["model.eval()\n","total_eval_accuracy = 0\n","total_eval_loss = 0\n","pred_list=[]\n","for batch in test_loader:\n","    with torch.no_grad():\n","        inputs = {key: val.to(device) for key, val in batch[0].items()}\n","        labels = batch[1].to(device)\n","        outputs = model(**inputs, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","    total_eval_loss += loss.item()\n","    preds = torch.argmax(logits, axis=1)\n","    pred_list.extend(list(preds.cpu().numpy()))\n","    eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n","#     print(eval_accuracy)\n","    total_eval_accuracy += eval_accuracy\n","\n","avg_eval_accuracy = total_eval_accuracy / len(test_loader)\n","avg_eval_loss = total_eval_loss / len(test_loader)\n","\n","print(f\"Epoch {epoch+1}:\")\n","print(f\"  Training loss: {loss.item():.4f}\")\n","print(f\"  Testing loss: {avg_eval_loss:.4f}\")\n","print(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T00:47:04.201704Z","iopub.status.busy":"2023-05-28T00:47:04.201255Z","iopub.status.idle":"2023-05-28T00:47:04.219511Z","shell.execute_reply":"2023-05-28T00:47:04.218521Z","shell.execute_reply.started":"2023-05-28T00:47:04.201658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.9826    0.9575    0.9699       471\n","           1     0.9585    0.9830    0.9706       470\n","\n","    accuracy                         0.9702       941\n","   macro avg     0.9705    0.9703    0.9702       941\n","weighted avg     0.9706    0.9702    0.9702       941\n","\n"]}],"source":["from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import classification_report\n","print(classification_report(test_y, pred_list, digits=4))"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T23:20:20.899690Z","iopub.status.busy":"2023-05-27T23:20:20.899290Z","iopub.status.idle":"2023-05-27T23:20:21.806222Z","shell.execute_reply":"2023-05-27T23:20:21.805145Z","shell.execute_reply.started":"2023-05-27T23:20:20.899660Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in ./config.json\n","Model weights saved in ./pytorch_model.bin\n","tokenizer config file saved in ./tokenizer_config.json\n","Special tokens file saved in ./special_tokens_map.json\n"]},{"data":{"text/plain":["('./tokenizer_config.json',\n"," './special_tokens_map.json',\n"," './vocab.json',\n"," './merges.txt',\n"," './added_tokens.json',\n"," './tokenizer.json')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"./\")\n","tokenizer.save_pretrained(\"./\")"]},{"cell_type":"markdown","metadata":{},"source":["# Test on stackoverflow"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T23:39:07.640550Z","iopub.status.busy":"2023-05-27T23:39:07.640052Z","iopub.status.idle":"2023-05-27T23:39:35.843244Z","shell.execute_reply":"2023-05-27T23:39:35.842046Z","shell.execute_reply.started":"2023-05-27T23:39:07.640511Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1:\n","  Training loss: 0.6224\n","  Testing loss: 0.4198\n","  Testing accuracy: 0.8484\n","              precision    recall  f1-score   support\n","\n","           0     0.8442    0.8553    0.8497       456\n","           1     0.8530    0.8418    0.8473       455\n","\n","    accuracy                         0.8485       911\n","   macro avg     0.8486    0.8485    0.8485       911\n","weighted avg     0.8486    0.8485    0.8485       911\n","\n"]}],"source":["df= pd.read_csv(data_file_path)\n","\n","df= df[df[\"origin\"]==\"StackOverflow\"]\n","df.reset_index(drop=True, inplace=True)\n","\n","from sklearn.model_selection import train_test_split\n","# human_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['post_answer']}\" , axis=1)\n","# chatgpt_answers = df.apply(lambda x: f\"Question: {x['post_question']} , Answer: {x['chatgpt-3.5-turbo']}\" , axis=1)\n","# human_answers= df[\"human_answer\"]\n","# chatgpt_answers = df[\"chatgpt_answer\"]\n","\n","processed_df= pd.DataFrame({\"text\": list(human_answers)+ list(chatgpt_answers), \"label\": [0]* len(human_answers) + [1]*len(chatgpt_answers)})\n","\n","train_val_df, test_df= train_test_split(processed_df, test_size = 0.1, stratify=processed_df.label, random_state=0)\n","train_df, val_df = train_test_split(train_val_df, test_size = 0.1, stratify=train_val_df.label, random_state=0)\n","\n","test_encodings = tokenizer(test_df.text.tolist(), truncation=True, padding=True, max_length=512)\n","test_y = list(test_df['label'])\n","test_dataset = TextClassificationDataset(test_encodings, test_y)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","\n","model.eval()\n","total_eval_accuracy = 0\n","total_eval_loss = 0\n","pred_list=[]\n","for batch in test_loader:\n","    with torch.no_grad():\n","        inputs = {key: val.to(device) for key, val in batch[0].items()}\n","        labels = batch[1].to(device)\n","        outputs = model(**inputs, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","    total_eval_loss += loss.item()\n","    preds = torch.argmax(logits, axis=1)\n","    pred_list.extend(list(preds.cpu().numpy()))\n","    eval_accuracy = torch.sum(preds == labels).item() / len(labels)\n","#     print(eval_accuracy)\n","    total_eval_accuracy += eval_accuracy\n","\n","avg_eval_accuracy = total_eval_accuracy / len(test_loader)\n","avg_eval_loss = total_eval_loss / len(test_loader)\n","\n","print(f\"Epoch {epoch+1}:\")\n","print(f\"  Training loss: {loss.item():.4f}\")\n","print(f\"  Testing loss: {avg_eval_loss:.4f}\")\n","print(f\"  Testing accuracy: {avg_eval_accuracy:.4f}\")\n","\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import classification_report\n","print(classification_report(test_y, pred_list, digits=4))"]},{"cell_type":"markdown","metadata":{},"source":["# Token perplexity"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T22:11:20.864253Z","iopub.status.busy":"2023-05-28T22:11:20.863335Z","iopub.status.idle":"2023-05-28T22:11:23.644556Z","shell.execute_reply":"2023-05-28T22:11:23.643276Z","shell.execute_reply.started":"2023-05-28T22:11:20.864218Z"},"trusted":true},"outputs":[],"source":["data_file_path = \"/kaggle/input/project/dataset.csv\"\n","df= pd.read_csv(data_file_path)\n","df= df[df[\"origin\"]==\"AskCulinary\"]\n","df.reset_index(drop=True, inplace=True)\n","\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","import torch\n","from tqdm import tqdm\n","\n","device = \"cuda\"\n","model_id = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n","tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n","\n","def calculate_perplexity(text):\n","    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        log_probabilities = outputs.logits\n","        n_tokens = log_probabilities.shape[1]\n","        print(log_probabilities.shape)\n","        avg_log_probabilities = log_probabilities.sum(dim=-1) / n_tokens\n","        print(avg_log_probabilities.shape)\n","        perplexity = torch.exp(avg_log_probabilities)\n","     \n","    return perplexity.item()\n","\n","texts = list(df.human_answer)\n","perplexities = [calculate_perplexity(text) for text in tqdm(texts)]\n","print(perplexities)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
